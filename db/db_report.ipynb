{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e98b43",
   "metadata": {},
   "source": [
    "# Database Options for an ASL Translator Microservice Application: A Comparative Analysis\n",
    "\n",
    "## 1. Project Context and Data Requirements\n",
    "\n",
    "Our project is a microservice-based American Sign Language (ASL) translator, consisting of the following components:\n",
    "\n",
    "- **API service**: A FastAPI application serving a web front-end and handling prediction requests.\n",
    "- **Inference service**: A Python-based model worker that processes images asynchronously via RabbitMQ.\n",
    "- **Message broker**: RabbitMQ, used for asynchronous communication between services.\n",
    "\n",
    "The data our application currently handles, or is likely to handle in the future, includes:\n",
    "\n",
    "- **Prediction logs**: Job ID, predicted gesture, confidence score, timestamp, latency, and potentially a user ID.\n",
    "- **User data**: User accounts, roles, preferences, and usage history.\n",
    "- **Feedback data**: User feedback indicating whether a prediction was correct or incorrect, including possible manual corrections.\n",
    "- **Model metadata**: Model versions, training timestamps, and performance metrics.\n",
    "- **Image data**: Either raw image data or references/paths to stored images.\n",
    "\n",
    "From these requirements, our database solution must support:\n",
    "\n",
    "- Structured, relational data (e.g., predictions, users, feedback).\n",
    "- Strong consistency to avoid conflicting states between predictions and feedback.\n",
    "- Good integration with Python/FastAPI and easy Docker-based deployment.\n",
    "- Potential extension toward analytics, monitoring, and model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Database Families Considered\n",
    "\n",
    "To determine the most suitable database solution for our project, we compared several database families:\n",
    "\n",
    "- **Relational SQL databases**: PostgreSQL, MySQL/MariaDB, SQLite.\n",
    "- **Document databases**: MongoDB.\n",
    "- **Wide-column / distributed databases**: Apache Cassandra.\n",
    "- **In-memory / cache stores**: Redis.\n",
    "- **Time-series databases**: InfluxDB and TimescaleDB.\n",
    "- **Vector databases (future-oriented)**: PostgreSQL with pgvector and dedicated vector databases such as Pinecone.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Relational SQL Databases\n",
    "\n",
    "### 3.1 PostgreSQL\n",
    "\n",
    "#### Why PostgreSQL is a strong candidate\n",
    "\n",
    "PostgreSQL aligns well with the relational nature of our data. Entities such as predictions, users, feedback, and models have clear relationships (one-to-many and many-to-many), which can be naturally modeled in relational tables.\n",
    "\n",
    "It also supports complex analytical queries, such as calculating average confidence per gesture per model version over a given time period, which is useful for monitoring model performance.\n",
    "\n",
    "PostgreSQL is a mature and reliable system, offering ACID-compliant transactions and strong consistency guarantees. It is widely used in production environments and benefits from robust tooling for backups, monitoring, and maintenance.\n",
    "\n",
    "From a development perspective, PostgreSQL integrates very well with FastAPI through tools such as SQLAlchemy and Alembic. There are numerous examples and tutorials for deploying FastAPI applications with PostgreSQL in Docker.\n",
    "\n",
    "Additionally, PostgreSQL supports advanced analytics features such as powerful indexing, window functions, and JSONB columns for semi-structured data. Extensions like TimescaleDB and pgvector allow PostgreSQL to be used as a hybrid time-series or vector database in the future.\n",
    "\n",
    "Finally, PostgreSQL is Docker-friendly, with official images that can be easily deployed alongside our microservices.\n",
    "\n",
    "#### Arguments against PostgreSQL\n",
    "\n",
    "Compared to SQLite, PostgreSQL introduces some operational overhead, as it requires running and managing a separate service. For very small or purely local projects, this may be unnecessarily complex. However, for our containerized microservice architecture, PostgreSQL remains a practical and scalable choice.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 MySQL / MariaDB\n",
    "\n",
    "#### Pros\n",
    "\n",
    "MySQL and MariaDB are mature relational databases with strong transaction support (via InnoDB). They provide similar benefits to PostgreSQL in terms of structured data, joins, and indexes. Both databases also have good Docker support and Python connectors.\n",
    "\n",
    "#### Cons / Why PostgreSQL is usually preferred\n",
    "\n",
    "MySQL and MariaDB are slightly less feature-rich than PostgreSQL, particularly for complex analytics, JSON handling, and advanced extensions. In addition, current Python/FastAPI communities and tutorials favor PostgreSQL as the default choice.\n",
    "\n",
    "**Conclusion**: MySQL/MariaDB are viable options, but PostgreSQL is generally preferred unless there is an existing MySQL-based infrastructure.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 SQLite\n",
    "\n",
    "#### Pros\n",
    "\n",
    "SQLite is extremely simple to set up, as it is file-based and does not require a separate server process. It supports full SQL and ACID compliance for single-node scenarios, making it ideal for quick local prototyping and unit testing. Its lightweight nature allows it to be easily bundled with the application.\n",
    "\n",
    "#### Cons\n",
    "\n",
    "SQLite is not ideal for multi-container microservice deployments because file-based databases do not scale well across containers. Concurrency is limited, and reliability can be an issue if multiple services attempt simultaneous writes. Migrating to a production database later may require code changes.\n",
    "\n",
    "**Conclusion**: SQLite is suitable for experiments and local development, but not recommended as the main database in a Dockerized microservice architecture like ours.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Document Databases\n",
    "\n",
    "### 4.1 MongoDB\n",
    "\n",
    "#### Potential advantages\n",
    "\n",
    "MongoDB offers a flexible schema, allowing us to store prediction results as JSON documents with variable fields. This flexibility is useful if our data evolves quickly or contains nested structures, such as multiple candidate predictions or model metadata in a single document.\n",
    "\n",
    "MongoDB can feel natural for developers who think in JSON. It also provides built-in support for horizontal scaling, sharding, and high availability.\n",
    "\n",
    "#### Arguments against MongoDB for our project\n",
    "\n",
    "Our core data is naturally relational. Relationships such as **users ↔ predictions ↔ feedback** are more efficiently and robustly handled in a relational database like PostgreSQL.  \n",
    "\n",
    "While modern MongoDB supports multi-document transactions, SQL databases remain more straightforward for relational, transactional workloads. Running a MongoDB cluster also introduces operational overhead that may be unnecessary for a small group project.\n",
    "\n",
    "**Conclusion**: MongoDB is viable, especially if schema flexibility is important, but PostgreSQL is generally a better fit for our core data.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Wide-Column / Distributed Databases\n",
    "\n",
    "### 5.1 Apache Cassandra\n",
    "\n",
    "#### Pros\n",
    "\n",
    "Cassandra is designed for high write throughput and horizontal scalability, making it suitable for extremely large datasets. It also provides fault tolerance across multi-node and multi-datacenter deployments.\n",
    "\n",
    "#### Cons\n",
    "\n",
    "For our project, Cassandra’s operational complexity is a significant drawback. Deployment, tuning, and maintenance are challenging, especially for a small team. Its data model requires careful design around query patterns, making ad-hoc queries less natural than in SQL databases. Moreover, the scale Cassandra is designed for exceeds the needs of our ASL translator.\n",
    "\n",
    "**Conclusion**: Cassandra is technically interesting but not recommended for this project due to its complexity and mismatch with our current requirements.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. In-Memory and Cache Stores\n",
    "\n",
    "### 6.1 Redis\n",
    "\n",
    "#### Best use-cases for our project\n",
    "\n",
    "- Caching recent predictions to speed up repeated requests.\n",
    "- Storing short-lived data such as session tokens, rate-limiting counters, or temporary job states.\n",
    "\n",
    "#### Arguments for Redis\n",
    "\n",
    "- Extremely fast reads and writes, as it is in-memory.\n",
    "- Simple key-value model, supporting lists, sets, and sorted sets.\n",
    "- Excellent Docker support and Python client libraries.\n",
    "\n",
    "#### Arguments against using Redis as the main DB\n",
    "\n",
    "- Not intended as a primary data store.\n",
    "- Persistence is optional and may risk data loss after a restart or failure.\n",
    "- Limited support for complex queries, joins, or analytics.\n",
    "\n",
    "**Conclusion**: Redis is suitable as a secondary component (cache or temporary storage), but not as a replacement for a relational database.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Time-Series Databases\n",
    "\n",
    "### 7.1 InfluxDB / TimescaleDB\n",
    "\n",
    "#### Potential uses\n",
    "\n",
    "- Monitoring and observability: tracking request counts, latency distributions, error rates, and model performance over time.\n",
    "- Model performance analytics: analyzing confidence and accuracy trends per model version.\n",
    "\n",
    "#### Arguments for\n",
    "\n",
    "- Optimized for time-series data with continuous queries, downsampling, and retention policies.\n",
    "- Integrates well with visualization tools like Grafana.\n",
    "\n",
    "#### Arguments against as the main DB\n",
    "\n",
    "- Not ideal for relational data such as users, predictions, and feedback.\n",
    "- Adds complexity to the architecture.\n",
    "\n",
    "**Conclusion**: Useful for monitoring or analytics at scale. For initial development, time-series metrics can be stored in PostgreSQL or Prometheus.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Vector Databases (Future-Oriented)\n",
    "\n",
    "If we later extend the ASL translator to:\n",
    "\n",
    "- Store gesture embeddings, or  \n",
    "- Perform semantic search (e.g., find similar gestures or map gestures to concepts),\n",
    "\n",
    "then a vector database could be useful. Examples include PostgreSQL with **pgvector**, Qdrant, Milvus, Pinecone, or Weaviate.\n",
    "\n",
    "Vector databases allow k-NN (nearest neighbor) searches in embedding space. For now, starting with PostgreSQL is sufficient, with the option to add pgvector or a separate vector DB later.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Comparative Summary\n",
    "\n",
    "| Database | Why | Why Not |\n",
    "|----------|-----|---------|\n",
    "| PostgreSQL | Best match for relational, transactional data. Strong FastAPI ecosystem. Good for analytics and Docker. | Slightly more setup than SQLite; overkill for small local-only prototypes. |\n",
    "| MySQL / MariaDB | Mature SQL DB; structured data, joins, indexes. | Fewer advanced features than PostgreSQL; Python/FastAPI examples favor PostgreSQL. |\n",
    "| SQLite | Very simple, file-based; ideal for quick local development/tests. | Not ideal for multi-container microservices; concurrency limits; not suitable long-term. |\n",
    "| MongoDB | Flexible schema; good for semi-structured JSON; evolves easily. | Core data is relational; SQL better for transactions/joins; extra operational overhead. |\n",
    "| Cassandra | High write throughput; fault-tolerant; massive scale. | Overkill for our scale; operationally complex; ad-hoc queries less natural. |\n",
    "| Redis | Great for caching, temporary states, sessions. | Not durable; limited querying; not suitable as primary DB. |\n",
    "| InfluxDB / TimescaleDB | Optimized for time-series metrics; monitoring; analytics. | Not suitable for relational core data; adds complexity. |\n",
    "| Vector DB | Useful if we store embeddings for semantic search. | Overkill for basic gesture classification and logging. |\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Recommendation for Our ASL Translator Project\n",
    "\n",
    "**Primary database (core choice):**\n",
    "\n",
    "- PostgreSQL for:\n",
    "  - Prediction logs (job ID, gesture, confidence, latency, timestamp, user, model version)\n",
    "  - Users and authentication data\n",
    "  - Feedback and corrections\n",
    "  - Model metadata and experiment tracking (basic level)\n",
    "\n",
    "**Secondary components (optional, as the project grows):**\n",
    "\n",
    "- Redis for caching and short-lived data.\n",
    "- Prometheus + Grafana, or PostgreSQL time-series tables, for metrics.\n",
    "- pgvector or a vector database if embeddings and similarity search are introduced.\n",
    "\n",
    "**Why this combination fits best:**\n",
    "\n",
    "- Aligns with our microservice and Docker architecture.\n",
    "- Matches our Python/FastAPI stack and common tutorials.\n",
    "- Balances simplicity (one main DB to learn and maintain) with flexibility (extensions for time-series and vector search).\n",
    "- Provides a solid foundation for both short-term coursework and potential long-term project evolution.\n",
    "\n",
    "## 11. Handling Image Data for Training and Testing\n",
    "\n",
    "### 11.1 Storage Options\n",
    "\n",
    "For our project, image data used in training and testing can be stored in two main ways:\n",
    "\n",
    "1. **As files in a filesystem or cloud storage, with references in the database**  \n",
    "   - Images are saved in a directory structure or cloud bucket (e.g., AWS S3, GCP Cloud Storage).  \n",
    "   - The database (PostgreSQL) stores **paths or URLs** to the images, along with metadata such as gesture label, user ID, timestamp, and dataset split (train/test/validation).  \n",
    "\n",
    "   **Advantages:**  \n",
    "   - Keeps the database small and efficient.  \n",
    "   - Easy to scale storage independently.  \n",
    "   - Works well with Docker and containerized microservices.\n",
    "\n",
    "   **Example schema in PostgreSQL:**\n",
    "   ```sql\n",
    "   CREATE TABLE images (\n",
    "       id SERIAL PRIMARY KEY,\n",
    "       user_id INT REFERENCES users(id),\n",
    "       gesture_label VARCHAR(10),\n",
    "       file_path TEXT NOT NULL,\n",
    "       dataset_split VARCHAR(10), -- 'train', 'test', or 'validation'\n",
    "       uploaded_at TIMESTAMP DEFAULT NOW()\n",
    "   );\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1a03c",
   "metadata": {},
   "source": [
    "### 11.2 Storing Images Directly in the Database (BLOBs)\n",
    "\n",
    "Images are stored as **binary large objects (BLOBs)** inside the database.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Everything is in one system; backup includes images.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Can significantly bloat the database.\n",
    "- Slower read/write for large datasets.\n",
    "- Harder to scale and serve images efficiently.\n",
    "\n",
    "**Conclusion:** For our ASL Translator project, we recommend **option 1**: storing images externally (filesystem or cloud) and saving references in PostgreSQL.\n",
    "\n",
    "---\n",
    "\n",
    "### 11.3 Tracking Dataset Versions\n",
    "\n",
    "Each training/testing dataset can be versioned in the database:\n",
    "\n",
    "- Include a `dataset_version` column in the `images` table.\n",
    "- Helps reproduce training runs and track which data was used for which model.\n",
    "\n",
    "```sql\n",
    "ALTER TABLE images ADD COLUMN dataset_version VARCHAR(20) DEFAULT 'v1';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ddca4",
   "metadata": {},
   "source": [
    "Combine this with MLflow to log:\n",
    "\n",
    "- Which dataset version was used for training each model.\n",
    "\n",
    "- Which model version corresponds to which image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eed6531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_tag(\"dataset_version\", \"v1\")\n",
    "# mlflow.log_artifact(\"training_data_manifest.csv\", artifact_path=\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91836e13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 11.4 Metadata and Annotations\n",
    "\n",
    "Besides the raw image path, we can store:\n",
    "\n",
    "- Gesture label (for supervised training)\n",
    "\n",
    "- Confidence scores (if predictions are logged)\n",
    "\n",
    "- User ID (optional, for personalized datasets)\n",
    "\n",
    "- Timestamps (for monitoring dataset growth)\n",
    "\n",
    "- Dataset split (train, validation, test)\n",
    "\n",
    "This ensures we can query images efficiently without touching the binary files themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5bd08",
   "metadata": {},
   "source": [
    "---\n",
    "## 11.5 Integration with Microservices\n",
    "\n",
    "Inference service: \n",
    "- Loads images via file paths for preprocessing and prediction.\n",
    "\n",
    "Training service: \n",
    "- Reads image paths from the database for batch training.\n",
    "\n",
    "Experiment tracking: \n",
    "- MLflow records dataset version and corresponding model version.\n",
    "\n",
    "This setup keeps the database lean, works with Docker, and maintains reproducibility, which is essential for collaborative development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
